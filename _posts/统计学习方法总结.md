### 各个模型的特点
##### 1. 感知机
  - 描述：对于一组二类分类数据，选择一个超平面将这组数据分隔，位于超平面两边的点分别被分为正、负两类。
  - 作用：用于二类分类。
  - 损失函数：$-\sum_{x_{i}\in M}^{} y_i(w*x_i+b)$，即误分类点到分离超平面的距离
  - 学习方法：极小化损失函数
    1. 原始形式：梯度下降法，使用误分类点逐步更新w和b。
    2. 对偶形式：利用Gram矩阵，逐步消除误分类点。
  - 特点：感知机学习时误分类次数有上限，且对于线性可分数据，感知机学习算法收敛。

##### 2. k近邻法，k-NN
  - 描述：对于一个训练数据集，对新的输入实例，在训练数据及中找到与该实例最近的k个实例，这k个实例中多数属于某个类，就把该实例分到这个类。
  - 作用：用于分类与回归。
  - 损失函数：无。采用的决策规则为多数表决，即经验风险最小化。
  - 学习方法：
    - kd树。将整个空间划分为多个矩形区域，每个区域对应1个节点。
  - 要素：k值的选择、距离度量、分类决策规则

##### 3. 朴素贝叶斯法
  - 描述：对于给定的数据集，首先基于特征条件独立假设学习输入和输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
  - 作用：分类
  - 损失函数：对数似然损失。决策方法：后验概率最大化，即期望风险最小化。
  - 学习方法：
    - 朴素贝叶斯算法。先计算先验概率以及条件概率，再计算后验概率，最后取后验概率最大值。
    - 贝叶斯估计。对条件概率进行贝叶斯估计，条件概率和先验概率的分子添加一个正数$\lambda$，分母则添加$K\lambda$。当lambda取值为1时，称作拉普拉斯平滑。
  - 特点：是典型的生成学习方法，即先学习联合概率分布，再求得后验概率分布。它的基本假设是条件独立性。

##### 4. 决策树
  - 描述：决策树由结点和有向边组成，结点中内部节点表示一个特征或属性，外部节点表示一个类。分类时，从根节点开始，对实例某个特征进行测试，根据测试结果将实例分配到其子节点；此时每个子节点对应着该特征的一个取值，递归分配，直到达到叶节点，最后将实例分到叶节点的类中。
  - 作用：分类与回归。
  - 损失函数：正则化的极大似然函数。学习时采用损失函数最小化策略。
  - 重要概念：熵、条件熵、信息增益、信息增益比
  - 关键：特征选择，决策树生成和决策树的剪枝。
  - 生成算法：
    1. ID3算法。
      - 核心：在决策树各个节点上应用信息增益准则选择特征。
      - 若信息增益小于阈值或者剩下节点同属一个类，则得到一个叶节点。
      - 缺陷：只有树的生成，容易产生过拟合。
    2. C4.5算法。
      - 采用信息增益比，而不是信息增益作为特征选择的准则。
  - 剪枝算法：
    - 通过极小化决策树整体的损失函数或者代价函数来实现。
    - 损失函数：t为叶节点，$N_t$ 为叶节点中实例个数。损失函数为：$C_{a}(T)=\sum_{t=1}^{|T|}N_tH_t(T) + \alpha |T|$
    - 计算叶节点缩回到其父节点之前和之后的整个树的损失函数，若缩回之后损失函数更小，则缩回。
  - CART算法，即分类与回归树算法。
    - 回归树的生成。选择最优切分变量和切分点，递归地对区域及子区域进行切分。直至满足停止条件。
    - 分类树的生成。采用基尼指数，对每个特征以及该特征的每个取值，选择基尼指数最小的特征及其对应的且分店作为最优特征和最优切分点。
    - CART剪枝。采用递归的方法，自上而下地对各内部节点计算C(T)，g(t)。对最小的g(t)对应的结点进行剪枝，并以多数表决法决定它所属的类。

##### 5. 逻辑斯谛回归与最大熵模型,LR
  - 逻辑斯谛回归：
    - 输出Y=1的对数几率是输入x的线性函数。
    - 模型采用对数似然函数作为目标函数，优化该目标函数。
    - 对于经过函数(wx+b)计算后结果大于0的数据，经过sigmod函数分类后为1，小于0的数据则分类为0。
    - 因为估计时使用的是极大似然，求极大似然的极大值，因而采用梯度下降时实际使用的为梯度上升。
    -
  - 最大熵模型：
    - 最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。
    - 最大熵模型的学习可以转化为约束最优化问题。即在限定条件下求使得条件熵达到最大值的P。采用求解对偶问题的方式，求解得到P和w。
  - 学习方法：
    - 改进的迭代尺度法，IIS
    - 拟牛顿法，BFGS
    - LR的知识点：http://www.cnblogs.com/ModifyRong/p/7739955.html
##### 6. 支持向量机,SVM
  - 描述：在特征空间中找到一个分离超平面，该超平面将特征空间划分为两部分，一部分为正类，一部分为负类，法向量指向的一侧为正类。
  - 作用：分类
  - 重要概念：函数间隔、几何间隔
  - 种类：
    1. 线性可分支持向量机。
      - 数据线性可分，间隔最大化。
      - 应用拉格朗日对偶性，求解对偶问题。
    2. 线性支持向量机。
      - 对每个数据点，设置一个松弛变量ε，约束条件变为$y_i(w*x_i+b)\geq 1-\xi _i$。目标函数变为$\frac{1}{2}\left \| w \right \|^{2}+C\sum_{i=1}^{N}\xi _i$。C>0称为惩罚参数。_
      - 通过转化为对偶问题，求得a的值，之后获得w和b的值。
      - 合页损失函数，线性支持向量机也可以解释为最小化合页损失函数。合页损失函数不仅要求分类正确，而且确信度足够高时损失才是0。
    3. 非线性支持向量机与核函数。
      - 核函数：可以将数据从低维转化到高维的函数。
      - 正定核：核函数对应的Gram矩阵是半正定矩阵，则该核函数为正定核。
      - 常用核函数：
        1. 多项式核函数
        2. 高斯核函数
        3. 字符串核函数
      - 学习算法：用核函数替代原需要优化的函数中的内积部分，求得最优解a，再选择a的一个正分量求得b，再构造决策函数。
  - 序列最小最优化算法，SMO
    - 用于求解凸二次规划的对偶问题，即SVM中的最优化问题。
    - 选择两个变量a，其余变量固定，将原问题转化为二次规划问题。
    - 先计算未经过剪辑时的解$a_{2}^{new,unc}$，再获得剪辑后的$a_{2}^{new}$，最后计算得到$a_{1}^{new}$。
    - 变量的选择：第一个变量为违反KKT条件最严重的样本点，第二个变量为使得a2有最大变化的样本点。
    - 每次更新完两个变量后，都要重新计算阈值b。
    - SMO算法的理解，所有的alfa都满足一个等式（累加和为0）和一个不等式（在0和c之间），可以逐步更新两个alfa从而使整个函数取得最大值。KKT条件给出了alfa和函数yg(x)的关系，可以从已知的alfa中找出不满足kkt条件的，然后找出|E1-E2|最大的，根据二者更新得到新的两个alfa。更新的过程中alfa值必须满足上面的等式和不等式。而更新过程中的$a_{2}^{new,unc}$可以通过$a_{2}^{new,unc} = a _2^{old} + \frac{y _2(E _1 - E _2)}{K_{11}+K_{22}-K_{12}}$得到。
##### 提升方法
##### EM算法
##### HMM
##### CRF

##### 常见优化方法
###### 梯度下降法
  - 一个很好的博文：https://www.cnblogs.com/pinard/p/5970503.html
  - 目的为求解参数的值，找到最符合所有样本点的拟合函数。
  - 当前点的梯度方向由所有的样本决定，在当前theta下，下一个theta的判断需要所有样本点的数据。
  - 对于批量梯度下降法，样本个数m，x为n维向量，一次迭代需要把m个样本全部带入计算，迭代一次计算量为m*n^2。
  - 随机梯度下降每次迭代只使用一个样本，迭代一次计算量为n2，当样本个数m很大的时候，随机梯度下降迭代一次的速度要远高于批量梯度下降方法。两者的关系可以这样理解：随机梯度下降方法以损失很小的一部分精确度和增加一定数量的迭代次数为代价，换取了总体的优化效率的提升。增加的迭代次数远远小于样本的数量。
  - 对批量梯度下降法和随机梯度下降法的总结：
    - 批量梯度下降---最小化所有训练样本的损失函数，使得最终求解的是全局的最优解，即求解的参数是使得风险函数最小，但是对于大规模样本问题效率低下。
    - 随机梯度下降---最小化每条样本的损失函数，虽然不是每次迭代得到的损失函数都向着全局最优方向， 但是大的整体的方向是向全局最优解的，最终的结果往往是在全局最优解附近，适用于大规模训练样本情况。

##### 牛顿法与拟牛顿法
  1. 牛顿法：
    - 从当前点选择切线，该切线与x轴的交点会更接近原函数图像的零点。
    - 牛顿法与梯度下降：
      - 从本质上去看，牛顿法是二阶收敛，梯度下降是一阶收敛，所以牛顿法就更快。如果更通俗地说的话，比如你想找一条最短的路径走到一个盆地的最底部，梯度下降法每次只从你当前所处位置选一个坡度最大的方向走一步，牛顿法在选择方向时，不仅会考虑坡度是否够大，还会考虑你走了一步之后，坡度是否会变得更大。所以，可以说牛顿法比梯度下降法看得更远一点，能更快地走到最底部。（牛顿法目光更加长远，所以少走弯路；相对而言，梯度下降法只考虑了局部的最优，没有全局思想。）
      - 根据wiki上的解释，从几何上说，牛顿法就是用一个二次曲面去拟合你当前所处位置的局部曲面，而梯度下降法是用一个平面去拟合当前的局部曲面，通常情况下，二次曲面的拟合会比平面更好，所以牛顿法选择的下降路径会更符合真实的最优下降路径。
    - 优点：二阶收敛，收敛速度快。
    - 缺点：每一步都需要求解目标函数的Hessian矩阵的逆矩阵，计算比较复杂。
  2. 拟牛顿法：
    - 选取一个正定矩阵G代替牛顿法中得到的H^-1。
    - 正定矩阵的计算方法有DFP,BFGS
  3. 可用于求解条件随机场和LR模型。

##### 集成方法
  1. bagging：
    - 主要通过对训练数据集进行随机采样，以重新组合成不同的数据集，利用弱学习算法对不同的新数据集进行学习，得到一系列的预测结果，对这些预测结果做平均或者投票做出最终的预测
    - 随机森林Random Forest算法
  2. Boosting
    - 通过对样本进行不同的赋值，对错误学习的样本的权重设置的较大，这样，在后续的学习中集中处理难学的样本，最终得到一系列的预测结果，每个预测结果有一个权重，较大的权重表示该预测效果较好
    - AdaBoost算法和GBDT
