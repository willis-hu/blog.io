---
layout: post
title: lr,svm,感知机对比
date: 2018-04-10
categories: 机器学习
tags: [机器学习]
description: 机器学习模型对比之lr,svm,感知机
---

最优化问题不同：
  - 感知机：误分类点到分离超平面的距离之和最小化。
  - LR:极大似然估计样本的后验概率分布，即用极大似然法估计样本被分为1类和0类的概率。
  - SVM:最大化支持向量到超平面之间的距离。

性能对比：
  - 感知机模型计算简单，只需要计算错误样本点和标签的乘积对参数进行更新。**在线性可分的数据集中收敛，但在线性不可分的数据集中不收敛。**
  - 逻辑斯蒂回归相比感知机多了一层sigmoid函数的计算，计算仍然十分高效。**在线性可分的数据集中不能收敛但可以加入正则化来使算法收敛，在线性不可分的数据集中可以较好的收敛。**
  - 在逻辑斯蒂回归中，“线性可分”这个条件对logistic regression算法和perceptron算法的影响是截然相反的。对于perceptron算法来说，“线性可分”是算法能够结束的充要条件，是件好事。**但对于logistic regression来说，“线性可分”却是件坏事，它意味着目标函数没有极小值——没错，目标函数是凸的，但存在一个方向，沿着这个方向走下去，目标函数是单调减小的（但有界）**。感性认识就是，既然线性可分，那我们就希望那个S型曲面尽可能陡峭，而某个方向的w越大，则该方向的投影图越陡峭，而陡峭是没有限度的。所以，在线性可分的数/据集上做logistic regression，就必须加regularization以保证算法收敛。
  - 支持向量机计算相对复杂，但因为其实 **凸优化** 问题，因此算法一定可以收敛。
  - 感知机模型无法加入 **核方法** 映射到高维，而逻辑斯蒂回归和支持向量机都能通过核方法对低维不可分高维可分的数据集进行分类。
  - 感知机模型和支持向量机模型可以通过二分类方法的扩展处理 **多分类** 问题，方法是对每一类都产生一个分离超平面区分该类和其他类的样本。逻辑斯蒂回归进行多分类问题通过选择最后一个类为基准类，构造k-1个分离超平面（分类器），k-1个分类器计算sigmoid函数值求和在加1作为投票法的分母，每个分类器计算的sigmoid函数值作为分子进行投票，选出最大的分配到该类 别。

LR的特征选择：
  - 进行特征选择时，由于逻辑斯蒂回归的优点，开始的时候不用考虑各个特征之间是否有相关性，直接把能用的特征全部线性加权起来就好。经过初步训练，观察各个特征的权值，如果权值接近为0，那么就可以将这个特征看做是不相关的可以去除的特征。总结起来就是：先做加法再做减法。

LR与SVM比较：
  - 直接用线性回归做分类因为考虑到了所有样本点到分类决策面的距离，所以在两类数据分布不均匀的时候将导致误差非常大；逻辑斯蒂回归回归和SVM克服了这个缺点，前者采用将所有数据采用sigmod函数进行了非线性映射，使得远离分类决策面的数据作用减弱；后者则直接去掉了远离分类决策面的数据，只考虑支持向量的影响。

LR的优点-by《机器学习》：
  - 它直接对分类可能性进行建模，无需事先假设数据分布，避免了假设分布不准确带来的问题。
  - 它不仅仅是预测类别，而是得到近似概率预测，对许多需要利用概率辅助决策的任务很有用。
  - 对数几率函数是任意阶可导的凸函数，有很好的数学性质。

对lr或者svm的模型选择问题：
  1. n = 特征数量，m = 训练样本数量1）如果n相对于m更大，比如 n = 10,000，m = 1,000，则使用lr。理由：特征数相对于训练样本数已经够大了，使用线性模型就能取得不错的效果，不需要过于复杂的模型；
  2. 如果n较小，m比较大，比如n = 10，m = 10,000，则使用SVM（高斯核函数）.理由：在训练样本数量足够大而特征数较小的情况下，可以通过使用复杂核函数的SVM来获得更好的预测性能，而且因为训练样本数量并没有达到百万级，使用复杂核函数的SVM也不会导致运算过慢；
  3. 如果n较小，m非常大，比如n = 100, m = 500,000，则应该引入／创造更多的特征，然后使用lr或者线性核函数的SVM理由：因为训练样本数量特别大，使用复杂核函数的SVM会导致运算很慢，因此应该考虑通过引入更多特征，然后使用线性核函数的SVM或者lr来构建预测性更好的模型。
