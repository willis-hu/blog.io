
- 一个很好的博文：[决策树之理解ID3算法和C4.5算法](https://blog.csdn.net/u014688145/article/details/53212112)

- 决策树的剪枝：[决策树之剪枝原理与CART算法](https://blog.csdn.net/u014688145/article/details/53326910)

- 后剪枝算法中，重点在于alpha的确定，对于一个已经生成的决策树，一个给定的alpha会对应着一个最优的子树。而判断最优子树的过程需要在验证集上进行，通过验证集在决策树上的分类准确率来确定选取哪一个最优子树。
- 选择alpha的过程是一种启发式的方法，首先要明确的是，alpha越大，则决策树中能剪枝的部分就越多，对于某个单节点子树Ct，若当前树为二叉树，且假设剪枝前该节点只有两个子节点，那么剪枝前的损失函数为：$C_a(T) = C(T)+\alpha |2|$，而剪枝后的损失函数为$C_a(t) = C(t)+\alpha |1|$，而进行剪枝的判断条件为$C_a(T)>C_a(t)$，带入可得$C(t)-C(T)<\alpha$，即若不确定数的减少小于 $\alpha$时进行剪枝，那么 $\alpha$很大，则需要剪枝的部分也就很多。所以对于一个已经生成的决策树，我们遍历所有的内部节点，选取最小的 $\alpha$值，就可以得到只进行一次剪枝的子树，再将该子树设为当前剪枝的输入，即可递归n个子树。最后在验证集上进行验证，选择分类效果最好的子树。
-
- 决策树的预剪枝可以包含以下几种：
  - 每一个结点所包含的最小样本数目，例如10，则该结点总样本数小于10时，则不再分；
  - 指定树的高度或者深度，例如树的最大深度为4；
  - 指定结点的熵小于某个值，不再划分。
