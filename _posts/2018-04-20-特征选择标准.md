#### 特征选择的作用：
  - 减少特征数量，使得模型的泛化能力更强，减少过拟合，也可以提高训练速度。
  - 增强对特征和特征值之间的理解。

#### 单变量特征选择的方法
  - 直接选择：
    - 对于特征值变化较小的特征，例如100个样本中有99个都是1，则可以考虑删除这个特征。该方法只适用于特征值都是离散值的情况，若特征值是连续值，则需要先将连续变量离散化才可以进行这种处理。该方法不常用，可以把它当做特征选择的预处理。
  - 皮尔逊系数：
    - 对于特征和label线性相关情况，可以采用皮尔逊系数。对于两个变量，计算它们的协方差，协方差越大可以认为二者的相关性较大。一个变量和它自己的协方差就是该变量的方差。但是由于变量的量级对于协方差的大小有很大的影响，因而不能根据协方差的大小判断不同变量组合之间的相关性，因而引入皮尔逊系数，将两组变量的协方差除以各自的标准差，得到的值在(-1,1)之间，变量和它自身的皮尔逊系数显然为1，两个变量之间计算的皮尔逊系数越接近1，表示两个变量的正相关性越强，越接近-1，表示变量之间的负相关性越强。越接近0表示相关性越弱。因而皮尔逊系数可以作为特征选择的一种方法。
    - 缺点：皮尔逊系数不适合处理非线性关系，对于非线性关系，即使两个变量具有一一对应的关系，皮尔逊相关性也会接近0。
    - 优点：计算速度很快，这在处理大规模数据时很重要。且皮尔逊系数取值的正负可以表示更丰富的相关关系，绝对值能够表示强度。
  - 互信息：![互信息公式](https://img-blog.csdn.net/20160209162030300?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
    - 互信息是用来评价一个事件的出现对另一个事件出现所贡献的信息量，具体的公式如上。实际做单特征选择时，可以把某个特征是否出现和分类是否正确者两个事件放在一起计算，保留得分较高的特征，即，保留对分类正确这一事件贡献较大的特征。
    - 缺点：互信息不属于度量方式，没有办法归一化，在不同数据集上的结果无法做比较。且对于连续变量的计算不是很方便，通常变量需要先离散化，而互信息的结果对离散化的方式比较敏感。
    - 改进：最大信息系数(MIC):针对两个变量之间的关系离散在二维空间中，且使用散点图表示，将当前二维空间在x,y方向划分一定的区间数，然后查看当前散点在各个方格中落入的情况，作为联合概率的计算。MIC公式为： $mic(x;y)=\max_{a*b<B}\frac{I(x;y)}{log_2\min(a,b)}$ a,b是在x，y方向上的划分格子的个数，B是变量，一般设置为数据量的0.6次方左右。
    - 优点：互信息可以应对非线性相关的情况，且该方法在样本足够大的时候能为不同类型的单噪声程度相似的相关系数给出相近的分数，例如对于一个充满噪声的线性关系和一个正弦关系，它能够给出相似的结果。MIC解决了归一化和连续数据度量的问题。
  - 卡方检验： ![卡方检验：](https://img-blog.csdn.net/20160209162246902?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center)
    - 其中E代表当两者独立时期望的数量，例如E11代表两个事件独立时，共同出现的期望值。具体的计算可以表示为：![卡方检验](https://img-blog.csdn.net/20160209162314402?watermark/2/text/aHR0cDovL2Jsb2cuY3Nkbi5uZXQv/font/5a6L5L2T/fontsize/400/fill/I0JBQkFCMA==/dissolve/70/gravity/Center) 卡方检验是基于独立性假设的检验方法，度量的是期望值E和观察值N的偏离程度。卡方值越大意味着独立性假设不成立。如果两个事件互相依赖，那么词项的出现也会使某个类别的出现更有可能或更不可能，因此它适合于作为特征被选出来。
    - 缺点：对于出现次数较少的特征更容易给出高分，例如某一个特征就出现过一次在分类正确的数据中，则该特征会得到相对高的分数，而互信息则给分较低。
#### 正则化
  - L1正则：L1正则化使得学习到的模型很稀疏，因而可以作为特征选择的一种方法。但是L1正则化像非正则化线性模型一样也是不稳定的，如果特征集合中具有相关联的特征，当数据发生细微变化时也有可能导致很大的模型差异。
  - 在gbdt中计算节点分裂时候的gini增益值或者最小二乘法增加值，增加值较大的特征重要性较大。可以通过feature_importance方法得到。

#### 随机森林:
  - 平均不纯度减少:
    - 决策树中的每一个节点都是关于某个特征的条件，为的是将数据集按照不同的响应变量一分为二。利用不纯度可以确定节点（最优条件），对于分类问题，通常采用基尼不纯度或者信息增益，对于回归问题，通常采用的是方差或者最小二乘拟合。当训练决策树的时候，可以计算出每个特征减少了多少树的不纯度。对于一个决策树森林来说，可以算出每个特征平均减少了多少不纯度，并把它平均减少的不纯度作为特征选择的值。
    - 缺点：1、这种方法存在偏向，对具有更多类别的变量会更有利；2、对于存在关联的多个特征，其中任意一个都可以作为指示器（优秀的特征），并且一旦某个特征被选择之后，其他特征的重要度就会急剧下降，因为不纯度已经被选中的那个特征降下来了，其他的特征就很难再降低那么多不纯度了，这样一来，只有先被选中的那个特征重要度很高，其他的关联特征重要度往往较低。在理解数据时，这就会造成误解，导致错误的认为先被选中的特征是很重要的，而其余的特征是不重要的，但实际上这些特征对响应变量的作用确实非常接近的。
  - 平均精确率减少：
    - 直接度量每个特征对模型精确率的影响。主要思路是打乱每个特征的特征值顺序，并且度量顺序变动对模型的精确率的影响。很明显，对于不重要的变量来说，打乱顺序对模型的精确率影响不会太大，但是对于重要的变量来说，打乱顺序就会降低模型的精确率。
  - 袋外数据误差：对每一颗树，计算它的袋外数据误差，即为error1,随机地对袋外数据的所有样本在特征X上添加噪声，再一次计算它的袋外数据误差，误差变动较大的特征即为较重要的特征，

#### 参考博客
  - [特征选择和特征解](https://blog.csdn.net/ivysister/article/details/51482917)
  - [互信息](https://www.cnblogs.com/gatherstars/p/6004075.html)
