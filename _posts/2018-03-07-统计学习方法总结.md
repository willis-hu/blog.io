---
layout: post
title: 统计学习方法学习总结
date: 2018-03-07
categories: blog
tags: [机器学习]
description: 回顾一下统计学习方法的内容
---

### 各个模型的特点
##### 1. 感知机
  - 描述：对于一组二类分类数据，选择一个超平面将这组数据分隔，位于超平面两边的点分别被分为正、负两类。
  - 作用：用于二类分类。
  - 损失函数：$ -\sum_{x_{i}\in M}^{} y_i(w*x_i+b) $，即误分类点到分离超平面的距离
  - 学习方法：极小化损失函数
    1. 原始形式：梯度下降法，使用误分类点逐步更新w和b。
    2. 对偶形式：利用Gram矩阵，逐步消除误分类点。
  - 特点：感知机学习时误分类次数有上限，且对于线性可分数据，感知机学习算法收敛。

##### 2. k近邻法，k-NN
  - 描述：对于一个训练数据集，对新的输入实例，在训练数据及中找到与该实例最近的k个实例，这k个实例中多数属于某个类，就把该实例分到这个类。
  - 作用：用于分类与回归。
  - 损失函数：无。采用的决策规则为多数表决，即经验风险最小化。
  - 学习方法：
    - kd树。将整个空间划分为多个矩形区域，每个区域对应1个节点。
  - 要素：k值的选择、距离度量、分类决策规则

##### 3. 朴素贝叶斯法
  - 描述：对于给定的数据集，首先基于特征条件独立假设学习输入和输出的联合概率分布，然后基于此模型，对给定的输入x，利用贝叶斯定理求出后验概率最大的输出y。
  - 作用：分类
  - 损失函数：对数似然损失。决策方法：后验概率最大化，即期望风险最小化。
  - 学习方法：
    - 朴素贝叶斯算法。先计算先验概率以及条件概率，再计算后验概率，最后取后验概率最大值。
    - 贝叶斯估计。对条件概率进行贝叶斯估计，条件概率和先验概率的分子添加一个正数$\lambda$，分母则添加$K\lambda$。当lambda取值为1时，称作拉普拉斯平滑。
  - 特点：是典型的生成学习方法，即先学习联合概率分布，再求得后验概率分布。它的基本假设是条件独立性。

##### 4. 决策树
  - 描述：决策树由结点和有向边组成，结点中内部节点表示一个特征或属性，外部节点表示一个类。分类时，从根节点开始，对实例某个特征进行测试，根据测试结果将实例分配到其子节点；此时每个子节点对应着该特征的一个取值，递归分配，直到达到叶节点，最后将实例分到叶节点的类中。
  - 作用：分类与回归。
  - 损失函数：正则化的极大似然函数。学习时采用损失函数最小化策略。
  - 重要概念：熵、条件熵、信息增益、信息增益比
  - 关键：特征选择，决策树生成和决策树的剪枝。
  - 生成算法：
    1. ID3算法。
      - 核心：在决策树各个节点上应用信息增益准则选择特征。
      - 若信息增益小于阈值或者剩下节点同属一个类，则得到一个叶节点。
      - 缺陷：只有树的生成，容易产生过拟合。
    2. C4.5算法。
      - 采用信息增益比，而不是信息增益作为特征选择的准则。
  - 剪枝算法：
    - 通过极小化决策树整体的损失函数或者代价函数来实现。
    - 损失函数：t为叶节点，$N_t$为叶节点中实例个数。损失函数为 $ C_{a}(T)=\sum_{t=1}^{|T|}N_tH_t(T) + \alpha |T| $
    - 计算叶节点缩回到其父节点之前和之后的整个树的损失函数，若缩回之后损失函数更小，则缩回。
  - CART算法，即分类与回归树算法。
    - 回归树的生成。选择最优切分变量和切分点，递归地对区域及子区域进行切分。直至满足停止条件。
    - 分类树的生成。采用基尼指数，对每个特征以及该特征的每个取值，选择基尼指数最小的特征及其对应的且分店作为最优特征和最优切分点。
    - CART剪枝。采用递归的方法，自上而下地对各内部节点计算C(T)，g(t)。对最小的g(t)对应的结点进行剪枝，并以多数表决法决定它所属的类。

##### 5. 逻辑斯谛回归与最大熵模型
  - 逻辑斯谛回归：
    - 输出Y=1的对数几率是输入x的线性函数。
    - 模型采用对数似然函数作为目标函数，优化该目标函数。
  - 最大熵模型：
    - 最大熵原理认为，学习概率模型时，在所有可能的概率模型中，熵最大的模型是最好的模型。
    - 最大熵模型的学习可以转化为约束最优化问题。即在限定条件下求使得条件熵达到最大值的P。采用求解对偶问题的方式，求解得到P和w。
