---
layout: post
title: Adaboost算法
date: 2018-04-09
categories: 机器学习
tags: [机器学习]
description: 机器学习模型之Adaboost
---

#### 基本思路：

 通过组合弱分类器得到强分类器。通过改变训练数据的概率分布，针对不同训练数据分布调用弱学习算法学习一系列弱分类器。

#### 两个问题：
1. 如何在每一轮改变训练数据的权重。
2. 如何将弱分类器组合成为强分类器。

#### 算法

1. 定义初始权重$w_{m,i}$，根据初始权重$D_m$找出基本分类器，即学习方法。
2. 计算基本分类器的分类误差率e，分类器的系数a。
3. 根据分类器的系数、初始权重计算改进的权重。计算方法为$W_{m+1,i} =  \frac{W_{m,i}}{Z_m}exp(-a_m y_i G_m(x_i)))$，获得调整后的权重。调整方法的目的为增加分类错误的样本的权重，减少分类正确样本的权重。每一次更新都是依据上一次的分类误差，而不是已获得的所有分类器。
4. 根据调整后的权重获得的分类器，在计算分类误差率e和系数a。
5. 在一定次数调整后，得到最终的分类器。$f(x)= \sum_{m=1}^{M}a_m G_m(x)$。

#### 算法的训练误差分析

三个公式：

1. $\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i) \leq \prod_mZ_m$，表示最终分类器的训练误差小于各个分类器中规范化因子的乘积。
2. $\prod_{m=1}^{M}Z_m \leq exp(-2\sum_{m=1}^{M}y_m^2),y_m = \frac{1}{2}-e_m$，由$Z_m$定义以及泰勒展开式推导得到不等式。
3. 若存在$y > 0$，对所有m有$y_m \geq y$，则有 $\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i) \leq exp(-2My^2)$，表示随着若分类器的增加，M增加，则分类器的误差会以指数形式减小。

#### 前向分布算法

主要思想：将加法模型中多个基函数各自分为单独的步骤计算，从后向前，依次计算各个基函数极其系数，逐步逼近最终的优化目标函数，可以达到简化计算复杂度的目的。

#### 具体方法：

1. 初始化 $f_0(x)=0$。
2. 将损失函数写作 $\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i:\gamma ))$，其中 $\beta$为基函数的系数，$\gamma$为基函数的参数， $f_{m-1}$ 是已知的，只需要对所有 $x_i$求得 $\beta$和 $\gamma$的最小值即可。根据得到的 $\beta$和 $\gamma$，可以更新模型 $f_m(x)=f_{m-1}(x) + \beta_mb(x; \gamma_m)$ 的值。
3. 逐次更新 $f_m(x)$ ，则可以得到最终的 $f(x)$ 的值。
4. 这个算法中主要是利用所有样本，逐步获得各个基函数的两个参数，已经获得的参数都会对之后的参数选取产生作用，因而可以逐步逼近最终的参数。

#### Adaboost算法用于回归

对于adaboost算法，一般都是用于分类问题，当应用于回归问题时有不同的分类误差率和模型权重计算方法。在应用于回归时候，先使得所有样本权重为1/m,对于第k轮学习，样本权重为 $w_{ki}$,该轮学习到的模型为 $G_k(x_i)$,计算该轮模型预测时的最大绝对误差 $|G_k(x_i)-y_i|$,记为 $E_{k}$,则该轮预测后某个样本的误差为 $e_{ki} = |G_k(x_i)-y_i|/E_{k}$,所有样本集合的预测误差为 $e_k =  \sum\limits_{i=1}^{m}w_{ki}e_{ki}$,即该轮模型的误差率。而该轮模型的权重系数 $\alpha$的计算公式为 $\alpha_k =\frac{e_k}{1-e_k}$，更新后的样本权重计算公式为 $w_{k+1,i} = \frac{w_{ki}}{Z_k}\alpha_k^{1-e_{ki}}$，同样的，Z为规范化因子。最后是模型结合策略，同样采用加权平均法，计算公式为 $f(x) = \sum\limits_{k=1}^{K}(ln\frac{1}{\alpha_k})G_k(x)$，则可以得到最终的回归器。

#### Adaboost算法的优缺点

Adaboost算法的优点有：
  1. 作为分类器时，分类精度很高。
  2. 在Adaboost的框架下，可以使用各种回归分类模型来构建弱学习器，非常灵活。
  3. 作为简单的二元分类器时，构造简单，结果可理解。
  4. 不容易发生过拟合

Adaboost算法的缺点为：
  1. 对异常样本敏感，异常样本在迭代中可能会获得较高的权重，影响最终的强学习器的预测准确性。
