星期三, 13. 十二月 2017 03:23下午

#### 基本思路：

 通过组合弱分类器得到强分类器。通过改变训练数据的概率分布，针对不同训练数据分布调用弱学习算法学习一系列弱分类器。

#### 两个问题：
1. 如何在每一轮改变训练数据的权重。
2. 如何将弱分类器组合成为强分类器。

#### 算法

1. 定义初始权重$w_{m,i}$，根据初始权重$D_m$找出基本分类器，即学习方法。
2. 计算基本分类器的分类误差率e，分类器的系数a。
3. 根据分类器的系数、初始权重计算改进的权重。计算方法为$W_{m+1,i} =  \frac{W_{m,i}}{Z_m}exp(-a_m y_i G_m(x_i)))$，获得调整后的权重。调整方法的目的为增加分类错误的样本的权重，减少分类正确样本的权重。每一次更新都是依据上一次的分类误差，而不是已获得的所有分类器。
4. 根据调整后的权重获得的分类器，在计算分类误差率e和系数a。
5. 在一定次数调整后，得到最终的分类器。$f(x)= \sum_{m=1}^{M}a_m G_m(x)$。

#### 算法的训练误差分析

三个公式：

1. $\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i) \leq \prod_mZ_m$，表示最终分类器的训练误差小于各个分类器中规范化因子的乘积。
2. $\prod_{m=1}^{M}Z_m \leq exp(-2\sum_{m=1}^{M}y_m^2),y_m = \frac{1}{2}-e_m$，由$Z_m$定义以及泰勒展开式推导得到不等式。
3. 若存在$y > 0$，对所有m有$y_m \geq y$，则有$\frac{1}{N}\sum_{i=1}^{N}I(G(x_i)\neq y_i) \leq exp(-2My^2)$，表示随着若分类器的增加，M增加，则分类器的误差会以指数形式减小。

#### 前向分布算法

主要思想：将加法模型中多个基函数各自分为单独的步骤计算，从后向前，依次计算各个基函数极其系数，逐步逼近最终的优化目标函数，可以达到简化计算复杂度的目的。

具体方法：

1. 初始化$f_0(x)=0$。
2. 将损失函数写作$\sum_{i=1}^{N}L(y_i,f_{m-1}(x_i)+\beta b(x_i:\gamma ))$，其中$ \beta$为基函数的系数，$ \gamma$为基函数的参数，$f_{m-1}$是已知的，只需要对所有$x_i$求得$ \beta$和$ \gamma$的最小值即可。根据得到的$ \beta$和$ \gamma$，可以更新模型$f_m(x)=f_{m-1}(x) + \beta_mb(x; \gamma_m)$的值。
3. 逐次更新$f_m(x)$，则可以得到最终的$f(x)$的值。
4. 这个算法中主要是利用所有样本，逐步获得各个基函数的两个参数，已经获得的参数都会对之后的参数选取产生作用，因而可以逐步逼近最终的参数。

#### 提升树
