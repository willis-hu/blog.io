---
layout: post
title: sklearn中的特征工程处理部分
date: 2018-04-15
categories: 机器学习
tags: [机器学习]
description: 特征工程处理
---


[使用sklearn做单机特征工程](http://www.cnblogs.com/jasonfreak/p/5448385.html)

##### 特征工程中需要注意的点：
  1. 特征使用方案，即使用特征的效果。
  2. 获取特征方案，即获取和存储方案。
  3. 特征处理，包括特征清洗和预处理。
  4. 特征监控，特征有效性权重分析等。

##### 数据预处理：
  1. 无量纲化。包括标准化和区间缩放法。标准化使用均值和标准差，使得标准化之后的数据服从零均值正态分布。区间缩放法则使用最大值和最小值，将转化后的数据投影到0-1空间中。
          from sklearn.preprocessing import StandardScaler，MinMaxScaler
          StandardScaler().fit_transform(iris.data)
          MinMaxScaler().fit_transform(iris.data)
  2. 对定量特征二值化。对于定量的特征，如果我们关注的只是这个特征值是否超过某个阈值，则可以将该特征二值化，大于该阈值的设置为1，小于该阈值的设置为0。
          from sklearn.preprocessing import Binarizer
          Binarizer(threshold=3).fit_transform(iris.data)
  3. 对定性特征哑编码。对于多类别的特征，可以对其进行one-hot编码，即对于n个值的某个特征，将其编码为n位，若某个值出现则对应位取值为1。
          from sklearn.preprocessing import OneHotEncoder
          OneHotEncoder().fit_transform(iris.target.reshape((-1,1)))
  4. 缺失值计算。
          from sklearn.preprocessing import Imputer
          Imputer().fit_transform(vstack((array([nan, nan, nan, nan]), iris.data)))
  5. 数据变换。

##### 特征选择：
  1. 选择有意义的特征输入模型中，主要考虑两个方面：
    1. 特征是否发散。
    2. 特征与目标的相关性。
  2. Filter法，按照发散性或者相关性对特征进行评分。
    1. 方差选择法，选择方差大于阈值的特征。
    2. 相关系数法，选择与目标值相关系数较大的特征。


##### lr和gpdt的对比：
  1. lr适合处理高维稀疏特征，因为lr中往往带着惩罚项，惩罚项为w的大小，因而对于稀疏特征，如果一个特征能够很好拟合分类数据，那么该特征的w就会很大，这时正则化就会给该模型很大的惩罚。而gbdt中避免过拟合的方法为节点个数，这时树只要根据该特征分裂成两个节点就可以达到了，但这个模型的方差必然会很大。因而可以思考，xgboost应该也可以处理高维稀疏特征。

##### 特征编码部分：
  1. labelTransform，将特征进行编码，对于由n个不同特征组成的m个值，lb.fit(X)将n个特征从0到n-1编码，然后在按照这个编码给m个特征赋值。
