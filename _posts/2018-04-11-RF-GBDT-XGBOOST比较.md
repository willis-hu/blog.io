---
layout: post
title: RF,GBDT,XGBOOST比较
date: 2018-04-11
categories: 机器学习
tags: [机器学习]
description: 机器学习模型对比之RF,GBDT,XGBOOST
---


1. [随机森林学习](https://blog.csdn.net/qq547276542/article/details/78304454)
    1. 采用bootstrap sampling的思想，无放回地从数据集中进行采样，获得M个训练集，证明得到某个样本不被采样的概率为1/e。
    2. 采用CART作为基学习器，节点分裂时进行特征选择，无放回地抽样获取K个特征，从K个特征中选择最优特征。
    3. 不进行剪枝操作。
    4. 在最终预测时候对于某个样本可以选择包外样本集包含该样本的学习器，对其结果进行投票表决或者求均值。
    5. 随机森林在学习时，对采样之后的数据使用完全分裂的方式建立决策树，这样决策树的一个叶子节点要么是无法继续分裂的，要么是里面的样本都指向同一个分类。而之前的两个随机采样的过程保证了随机性，所以就算不剪枝，也不会出现过拟合。
    5. 随机森林的优点：
        1. 在数据集上表现良好
        1. 在当前的很多数据集上，相对其他算法有着很大的优势
        3. 它能够处理很高维度（feature很多）的数据，并且不用做特征选择
        4. 在训练完后，它能够给出哪些feature比较重要
        5. 在创建随机森林的时候，对generlization error使用的是无偏估计
        6. 训练速度快
        7. 在训练过程中，能够检测到feature间的互相影响
        8. 容易做成并行化方法
        9. 实现比较简单
    6. 随机森林的缺点：
        1. 当随机森林中的决策树个数很多时，训练时需要的空间和时间会较大
        2. 随机森林模型还有许多不好解释的地方，有点算个黑盒模型

2. GBDT：
  - [机器学习-一文理解GBDT的原理](https://zhuanlan.zhihu.com/p/29765582)
  - [GBDT算法原理深入解析](https://www.zybuluo.com/yxd/note/611571)
  - gbdt通过多轮迭代,每轮迭代产生一个**弱分类器**，每个分类器在上一轮分类器的**残差**基础上进行训练。对弱分类器的要求一般是足够简单，并且是**低方差和高偏差**的。因为训练的过程是通过**降低偏差**来不断提高最终分类器的精度
  - [梯度提升树(GBDT)原理小结](https://www.cnblogs.com/pinard/p/6140514.html)
  - [GBDT用于分类](http://www.cnblogs.com/leftnoteasy/archive/2011/03/07/random-forest-and-gbdt.htm)
  - gbdt用于分类时，先将y的值从类别值转化为向量，对于y所属的类对应轴上值为1，其余类值为0。如5类中属于第三类，则向量为(0,0,1,0,0)，放到gbdt中进行回归，输出值为对应的回归值，将输出值进行归一化，得到对应的属于某个类的概率，将该概率向量与真实向量相减，得到残差，再用新的模型拟合这个残差。

3. XGBOOST
  - [xgboost的原理没你想像的那么难](https://www.jianshu.com/p/7467e616f227)
  - gbdt拟合的数据为损失函数在当前模型负梯度上的值，实际为对当前模型函数求一阶导。而xgboost计算时，是根据子节点分离后的最大收益来确定是否分裂，之后求损失函数在当前f上的二阶泰勒展开，当此二阶泰勒展开有最小值时取到该新生成的树的最优节点取值。经过计算后的节点取值可以理解为该节点上数据x的一阶导除以二阶导和lambda的和的负方向。
  - [gbdt面试常考点](https://www.cnblogs.com/ModifyRong/p/7744987.html)
  - Xgboost相对于GBDT的优化：
    - 损失函数部分：
      - xgboost支持自定义损失函数
      - xgboost拟合的是损失函数对于新建的树对应的函数空间的二阶泰勒展开值。
      - xgboost添加了L1正则和L2正则
    - 树模型部分：
      - 叶节点的值是计算得到的，不同于一般的均值。
      - 节点分裂时的最佳分裂点选择可以并行执行。
      - 对于带缺失值的样本，可以自动计算样本所属的叶节点。
      - xgboost支持后剪枝。
      - 采用shrinkage思想，对新建的树加权。
    - 特征部分：
      - xgboost支持列采样。
      - xgboost支持设定样本权重。
4. 对比
    1. GBDT和随机森林的相同点：
        1. 都是由多棵树组成
        2. 最终的结果都是由多棵树一起决定
    3. GBDT和随机森林的不同点：
        1. 组成随机森林的树可以是分类树，也可以是回归树；而GBDT只由回归树组成
        2. 组成随机森林的树可以并行生成；而GBDT只能是串行生成
        3. 对于最终的输出结果而言，随机森林采用多数投票等；而GBDT则是将所有结果累加起来，或者加权累加起来
        4. 随机森林对异常值不敏感，GBDT对异常值非常敏感
        5. 随机森林对训练集一视同仁，GBDT是基于权值的弱分类器的集成
        6. 随机森林是通过减少模型方差提高性能，GBDT是通过减少模型偏差提高性能
